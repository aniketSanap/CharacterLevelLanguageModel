{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZFpRmnwvW1Xq",
    "outputId": "08ed9b75-5433-4559-8c6a-a87b3f1f7ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-e-ESNFWesG"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from string import printable\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6X3rJMsqU3z"
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, n_hidden_layers=3, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.legal_chars = printable\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            len(self.legal_chars), \n",
    "            hidden_size, \n",
    "            n_hidden_layers, \n",
    "            dropout=dropout_prob, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, len(self.legal_chars))\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.to(self.device)\n",
    "        lstm_output, hidden = self.lstm_layer(x, hidden)\n",
    "        output = self.dropout_layer(lstm_output)\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        return self.fc(output), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_hidden_layers, batch_size, self.hidden_size).zero_().to(self.device),\n",
    "                  weight.new(self.n_hidden_layers, batch_size, self.hidden_size).zero_().to(self.device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "VtgBw246WesN",
    "outputId": "608519f4-5429-41c7-99c2-586573f1dc14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "by Sir Arthur Conan Doyle\n",
      "(#15 in our series by Sir Arthur Conan \n",
      "\n",
      "[55 17 14 94 51 27 24 19 14 12 29 94 42 30 29 14 23 11 14 27 16 94 40 37\n",
      " 24 24 20 94 24 15 94 55 17 14 94 36 13 31 14 23 29 30 27 14 28 94 24 15\n",
      " 94 54 17 14 27 21 24 12 20 94 43 24 21 22 14 28 96 11 34 94 54 18 27 94\n",
      " 36 27 29 17 30 27 94 38 24 23 10 23 94 39 24 34 21 14 96 69 64  1  5 94\n",
      " 18 23 94 24 30 27 94 28 14 27 18 14 28 94 11 34 94 54 18 27 94 36 27 29\n",
      " 17 30 27 94 38 24 23 10 23 94]\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 128\n",
    "int_to_char = dict(enumerate(printable))\n",
    "char_to_int = {char:int_ for int_, char in int_to_char.items()}\n",
    "file_path = '/content/drive/My Drive/Colab Notebooks/Practice/CharacterGeneration/file.txt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:130])\n",
    "encoded = np.array([char_to_int[ch] for ch in text])\n",
    "print()\n",
    "print(encoded[:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "FaSNvFRMXX8A",
    "outputId": "4b50215a-c077-4bf2-b58e-1e2644eeb2cb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3445b70613fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0marr_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded' is not defined"
     ]
    }
   ],
   "source": [
    "def one_hot_encoder(arr):\n",
    "    \"\"\"\n",
    "    Input list_ contains indices of elements containing 1\n",
    "    \"\"\"\n",
    "    n_labels = len(printable)\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_batches(arr=encoded, batch_size=BATCH_SIZE, sequence_length=SEQ_LENGTH):\n",
    "    arr_len = len(arr)\n",
    "    num_batches = arr_len // (batch_size * sequence_length)\n",
    "\n",
    "    arr = arr[:num_batches * batch_size*sequence_length]\n",
    "    arr = arr.reshape((num_batches, batch_size, sequence_length))\n",
    "    for i in range(0, num_batches):\n",
    "        x = arr[i]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        try:\n",
    "            y[:, -1] = arr[i + 1][:, 0]\n",
    "            yield x, y\n",
    "        except Exception as e:\n",
    "            print('Exception at index:', i)\n",
    "\n",
    "batches = create_batches()\n",
    "x, y = next(batches)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "8KLae7UXiD6D",
    "outputId": "26a17ef0-1ab9-49af-d973-6b17193742f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLSTM(\n",
       "  (lstm_layer): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout_layer): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharLSTM()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHUOLIe_v_d2"
   },
   "outputs": [],
   "source": [
    "def train(net=model, data=encoded, epochs=20, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH, lr=0.001, clip=5, print_every=100):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in create_batches():\n",
    "            counter += 1\n",
    "            x = one_hot_encoder(x)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            targets = targets.to(device)\n",
    "            h = tuple([each.data for each in h])\n",
    "            opt.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()))\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ov7qjt020739",
    "outputId": "a09fdde6-f208-4867-d9e4-f3c22581a0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 100... Loss: 1.4962...\n",
      "Epoch: 1/50... Step: 200... Loss: 1.4632...\n",
      "Epoch: 1/50... Step: 300... Loss: 1.4616...\n",
      "Epoch: 1/50... Step: 400... Loss: 1.4414...\n",
      "Exception at index: 494\n",
      "Epoch: 2/50... Step: 500... Loss: 1.5595...\n",
      "Epoch: 2/50... Step: 600... Loss: 1.5143...\n",
      "Epoch: 2/50... Step: 700... Loss: 1.3982...\n",
      "Epoch: 2/50... Step: 800... Loss: 1.5132...\n",
      "Epoch: 2/50... Step: 900... Loss: 1.4703...\n",
      "Exception at index: 494\n",
      "Epoch: 3/50... Step: 1000... Loss: 1.4686...\n",
      "Epoch: 3/50... Step: 1100... Loss: 1.5354...\n",
      "Epoch: 3/50... Step: 1200... Loss: 1.4487...\n",
      "Epoch: 3/50... Step: 1300... Loss: 1.4907...\n",
      "Epoch: 3/50... Step: 1400... Loss: 1.4373...\n",
      "Exception at index: 494\n",
      "Epoch: 4/50... Step: 1500... Loss: 1.5473...\n",
      "Epoch: 4/50... Step: 1600... Loss: 1.5278...\n",
      "Epoch: 4/50... Step: 1700... Loss: 1.4304...\n",
      "Epoch: 4/50... Step: 1800... Loss: 1.4705...\n",
      "Epoch: 4/50... Step: 1900... Loss: 1.4154...\n",
      "Exception at index: 494\n",
      "Epoch: 5/50... Step: 2000... Loss: 1.5524...\n",
      "Epoch: 5/50... Step: 2100... Loss: 1.4390...\n",
      "Epoch: 5/50... Step: 2200... Loss: 1.3699...\n",
      "Epoch: 5/50... Step: 2300... Loss: 1.4984...\n",
      "Epoch: 5/50... Step: 2400... Loss: 1.5101...\n",
      "Exception at index: 494\n",
      "Epoch: 6/50... Step: 2500... Loss: 1.4724...\n",
      "Epoch: 6/50... Step: 2600... Loss: 1.5071...\n",
      "Epoch: 6/50... Step: 2700... Loss: 1.4268...\n",
      "Epoch: 6/50... Step: 2800... Loss: 1.4989...\n",
      "Epoch: 6/50... Step: 2900... Loss: 1.4755...\n",
      "Exception at index: 494\n",
      "Epoch: 7/50... Step: 3000... Loss: 1.4732...\n",
      "Epoch: 7/50... Step: 3100... Loss: 1.5042...\n",
      "Epoch: 7/50... Step: 3200... Loss: 1.3332...\n",
      "Epoch: 7/50... Step: 3300... Loss: 1.4727...\n",
      "Epoch: 7/50... Step: 3400... Loss: 1.4393...\n",
      "Exception at index: 494\n",
      "Epoch: 8/50... Step: 3500... Loss: 1.4492...\n",
      "Epoch: 8/50... Step: 3600... Loss: 1.4262...\n",
      "Epoch: 8/50... Step: 3700... Loss: 1.6347...\n",
      "Epoch: 8/50... Step: 3800... Loss: 1.3821...\n",
      "Epoch: 8/50... Step: 3900... Loss: 1.4046...\n",
      "Exception at index: 494\n",
      "Epoch: 9/50... Step: 4000... Loss: 1.6307...\n",
      "Epoch: 9/50... Step: 4100... Loss: 1.5468...\n",
      "Epoch: 9/50... Step: 4200... Loss: 1.6325...\n",
      "Epoch: 9/50... Step: 4300... Loss: 1.5715...\n",
      "Epoch: 9/50... Step: 4400... Loss: 1.4376...\n",
      "Exception at index: 494\n",
      "Epoch: 10/50... Step: 4500... Loss: 1.5221...\n",
      "Epoch: 10/50... Step: 4600... Loss: 1.5819...\n",
      "Epoch: 10/50... Step: 4700... Loss: 1.5678...\n",
      "Epoch: 10/50... Step: 4800... Loss: 1.4471...\n",
      "Epoch: 10/50... Step: 4900... Loss: 1.4727...\n",
      "Exception at index: 494\n",
      "Epoch: 11/50... Step: 5000... Loss: 1.5358...\n",
      "Epoch: 11/50... Step: 5100... Loss: 1.4985...\n",
      "Epoch: 11/50... Step: 5200... Loss: 1.4884...\n",
      "Epoch: 11/50... Step: 5300... Loss: 1.4389...\n",
      "Epoch: 11/50... Step: 5400... Loss: 1.5026...\n",
      "Exception at index: 494\n",
      "Epoch: 12/50... Step: 5500... Loss: 1.5422...\n",
      "Epoch: 12/50... Step: 5600... Loss: 1.4859...\n",
      "Epoch: 12/50... Step: 5700... Loss: 1.4761...\n",
      "Epoch: 12/50... Step: 5800... Loss: 1.3929...\n",
      "Epoch: 12/50... Step: 5900... Loss: 1.4108...\n",
      "Exception at index: 494\n",
      "Epoch: 13/50... Step: 6000... Loss: 1.4490...\n",
      "Epoch: 13/50... Step: 6100... Loss: 1.4596...\n",
      "Epoch: 13/50... Step: 6200... Loss: 1.5516...\n",
      "Epoch: 13/50... Step: 6300... Loss: 1.4559...\n",
      "Epoch: 13/50... Step: 6400... Loss: 1.4084...\n",
      "Exception at index: 494\n",
      "Epoch: 14/50... Step: 6500... Loss: 1.4542...\n",
      "Epoch: 14/50... Step: 6600... Loss: 1.4958...\n",
      "Epoch: 14/50... Step: 6700... Loss: 1.4816...\n",
      "Epoch: 14/50... Step: 6800... Loss: 1.4586...\n",
      "Epoch: 14/50... Step: 6900... Loss: 1.5127...\n",
      "Exception at index: 494\n",
      "Epoch: 15/50... Step: 7000... Loss: 1.4793...\n",
      "Epoch: 15/50... Step: 7100... Loss: 1.3925...\n",
      "Epoch: 15/50... Step: 7200... Loss: 1.4173...\n",
      "Epoch: 15/50... Step: 7300... Loss: 1.4749...\n",
      "Epoch: 15/50... Step: 7400... Loss: 1.4742...\n",
      "Exception at index: 494\n",
      "Epoch: 16/50... Step: 7500... Loss: 1.5412...\n",
      "Epoch: 16/50... Step: 7600... Loss: 1.4748...\n",
      "Epoch: 16/50... Step: 7700... Loss: 1.4185...\n",
      "Epoch: 16/50... Step: 7800... Loss: 1.4545...\n",
      "Epoch: 16/50... Step: 7900... Loss: 1.4410...\n",
      "Exception at index: 494\n",
      "Epoch: 17/50... Step: 8000... Loss: 1.5227...\n",
      "Epoch: 17/50... Step: 8100... Loss: 1.4308...\n",
      "Epoch: 17/50... Step: 8200... Loss: 1.4526...\n",
      "Epoch: 17/50... Step: 8300... Loss: 1.4217...\n",
      "Exception at index: 494\n",
      "Epoch: 18/50... Step: 8400... Loss: 1.5299...\n",
      "Epoch: 18/50... Step: 8500... Loss: 1.4342...\n",
      "Epoch: 18/50... Step: 8600... Loss: 1.4161...\n",
      "Epoch: 18/50... Step: 8700... Loss: 1.4313...\n",
      "Epoch: 18/50... Step: 8800... Loss: 1.4277...\n",
      "Exception at index: 494\n",
      "Epoch: 19/50... Step: 8900... Loss: 1.5055...\n",
      "Epoch: 19/50... Step: 9000... Loss: 1.4285...\n",
      "Epoch: 19/50... Step: 9100... Loss: 1.4382...\n",
      "Epoch: 19/50... Step: 9200... Loss: 1.4433...\n",
      "Epoch: 19/50... Step: 9300... Loss: 1.4336...\n",
      "Exception at index: 494\n",
      "Epoch: 20/50... Step: 9400... Loss: 1.4826...\n",
      "Epoch: 20/50... Step: 9500... Loss: 1.4789...\n",
      "Epoch: 20/50... Step: 9600... Loss: 1.5062...\n",
      "Epoch: 20/50... Step: 9700... Loss: 1.4473...\n",
      "Epoch: 20/50... Step: 9800... Loss: 1.4655...\n",
      "Exception at index: 494\n",
      "Epoch: 21/50... Step: 9900... Loss: 1.4452...\n",
      "Epoch: 21/50... Step: 10000... Loss: 1.4671...\n",
      "Epoch: 21/50... Step: 10100... Loss: 1.3735...\n",
      "Epoch: 21/50... Step: 10200... Loss: 1.5044...\n",
      "Epoch: 21/50... Step: 10300... Loss: 1.4129...\n",
      "Exception at index: 494\n",
      "Epoch: 22/50... Step: 10400... Loss: 1.4666...\n",
      "Epoch: 22/50... Step: 10500... Loss: 1.4941...\n",
      "Epoch: 22/50... Step: 10600... Loss: 1.3574...\n",
      "Epoch: 22/50... Step: 10700... Loss: 1.4794...\n",
      "Epoch: 22/50... Step: 10800... Loss: 1.5182...\n",
      "Exception at index: 494\n",
      "Epoch: 23/50... Step: 10900... Loss: 1.4568...\n",
      "Epoch: 23/50... Step: 11000... Loss: 1.4315...\n",
      "Epoch: 23/50... Step: 11100... Loss: 1.3513...\n",
      "Epoch: 23/50... Step: 11200... Loss: 1.4639...\n",
      "Epoch: 23/50... Step: 11300... Loss: 1.4213...\n",
      "Exception at index: 494\n",
      "Epoch: 24/50... Step: 11400... Loss: 1.4286...\n",
      "Epoch: 24/50... Step: 11500... Loss: 1.5213...\n",
      "Epoch: 24/50... Step: 11600... Loss: 1.4118...\n",
      "Epoch: 24/50... Step: 11700... Loss: 1.4189...\n",
      "Epoch: 24/50... Step: 11800... Loss: 1.4328...\n",
      "Exception at index: 494\n",
      "Epoch: 25/50... Step: 11900... Loss: 1.4349...\n",
      "Epoch: 25/50... Step: 12000... Loss: 1.5133...\n",
      "Epoch: 25/50... Step: 12100... Loss: 1.6110...\n",
      "Epoch: 25/50... Step: 12200... Loss: 1.4685...\n",
      "Epoch: 25/50... Step: 12300... Loss: 1.4863...\n",
      "Exception at index: 494\n",
      "Epoch: 26/50... Step: 12400... Loss: 1.6070...\n",
      "Epoch: 26/50... Step: 12500... Loss: 1.5072...\n",
      "Epoch: 26/50... Step: 12600... Loss: 1.5740...\n",
      "Epoch: 26/50... Step: 12700... Loss: 1.4296...\n",
      "Epoch: 26/50... Step: 12800... Loss: 1.4455...\n",
      "Exception at index: 494\n",
      "Epoch: 27/50... Step: 12900... Loss: 1.5023...\n",
      "Epoch: 27/50... Step: 13000... Loss: 1.5519...\n",
      "Epoch: 27/50... Step: 13100... Loss: 1.5635...\n",
      "Epoch: 27/50... Step: 13200... Loss: 1.4230...\n",
      "Epoch: 27/50... Step: 13300... Loss: 1.4384...\n",
      "Exception at index: 494\n",
      "Epoch: 28/50... Step: 13400... Loss: 1.5065...\n",
      "Epoch: 28/50... Step: 13500... Loss: 1.4606...\n",
      "Epoch: 28/50... Step: 13600... Loss: 1.5257...\n",
      "Epoch: 28/50... Step: 13700... Loss: 1.3903...\n",
      "Epoch: 28/50... Step: 13800... Loss: 1.4439...\n",
      "Exception at index: 494\n",
      "Epoch: 29/50... Step: 13900... Loss: 1.4514...\n",
      "Epoch: 29/50... Step: 14000... Loss: 1.5270...\n",
      "Epoch: 29/50... Step: 14100... Loss: 1.4978...\n",
      "Epoch: 29/50... Step: 14200... Loss: 1.5010...\n",
      "Epoch: 29/50... Step: 14300... Loss: 1.3920...\n",
      "Exception at index: 494\n",
      "Epoch: 30/50... Step: 14400... Loss: 1.4573...\n",
      "Epoch: 30/50... Step: 14500... Loss: 1.4771...\n",
      "Epoch: 30/50... Step: 14600... Loss: 1.4425...\n",
      "Epoch: 30/50... Step: 14700... Loss: 1.4546...\n",
      "Epoch: 30/50... Step: 14800... Loss: 1.3535...\n",
      "Exception at index: 494\n",
      "Epoch: 31/50... Step: 14900... Loss: 1.5105...\n",
      "Epoch: 31/50... Step: 15000... Loss: 1.4610...\n",
      "Epoch: 31/50... Step: 15100... Loss: 1.4083...\n",
      "Epoch: 31/50... Step: 15200... Loss: 1.4427...\n",
      "Epoch: 31/50... Step: 15300... Loss: 1.4266...\n",
      "Exception at index: 494\n",
      "Epoch: 32/50... Step: 15400... Loss: 1.5279...\n",
      "Epoch: 32/50... Step: 15500... Loss: 1.4123...\n",
      "Epoch: 32/50... Step: 15600... Loss: 1.4410...\n",
      "Epoch: 32/50... Step: 15700... Loss: 1.3802...\n",
      "Epoch: 32/50... Step: 15800... Loss: 1.3926...\n",
      "Exception at index: 494\n",
      "Epoch: 33/50... Step: 15900... Loss: 1.4972...\n",
      "Epoch: 33/50... Step: 16000... Loss: 1.4700...\n",
      "Epoch: 33/50... Step: 16100... Loss: 1.4772...\n",
      "Epoch: 33/50... Step: 16200... Loss: 1.3565...\n",
      "Epoch: 33/50... Step: 16300... Loss: 1.3965...\n",
      "Exception at index: 494\n",
      "Epoch: 34/50... Step: 16400... Loss: 1.4496...\n",
      "Epoch: 34/50... Step: 16500... Loss: 1.3865...\n",
      "Epoch: 34/50... Step: 16600... Loss: 1.4560...\n",
      "Epoch: 34/50... Step: 16700... Loss: 1.4836...\n",
      "Exception at index: 494\n",
      "Epoch: 35/50... Step: 16800... Loss: 1.4915...\n",
      "Epoch: 35/50... Step: 16900... Loss: 1.4555...\n",
      "Epoch: 35/50... Step: 17000... Loss: 1.3714...\n",
      "Epoch: 35/50... Step: 17100... Loss: 1.4314...\n",
      "Epoch: 35/50... Step: 17200... Loss: 1.4723...\n",
      "Exception at index: 494\n",
      "Epoch: 36/50... Step: 17300... Loss: 1.5008...\n",
      "Epoch: 36/50... Step: 17400... Loss: 1.5448...\n",
      "Epoch: 36/50... Step: 17500... Loss: 1.3896...\n",
      "Epoch: 36/50... Step: 17600... Loss: 1.4499...\n",
      "Epoch: 36/50... Step: 17700... Loss: 1.4038...\n",
      "Exception at index: 494\n",
      "Epoch: 37/50... Step: 17800... Loss: 1.4805...\n",
      "Epoch: 37/50... Step: 17900... Loss: 1.4821...\n",
      "Epoch: 37/50... Step: 18000... Loss: 1.4506...\n",
      "Epoch: 37/50... Step: 18100... Loss: 1.4496...\n",
      "Epoch: 37/50... Step: 18200... Loss: 1.3948...\n",
      "Exception at index: 494\n",
      "Epoch: 38/50... Step: 18300... Loss: 1.4646...\n",
      "Epoch: 38/50... Step: 18400... Loss: 1.5061...\n",
      "Epoch: 38/50... Step: 18500... Loss: 1.4162...\n",
      "Epoch: 38/50... Step: 18600... Loss: 1.4193...\n",
      "Epoch: 38/50... Step: 18700... Loss: 1.4266...\n",
      "Exception at index: 494\n",
      "Epoch: 39/50... Step: 18800... Loss: 1.4277...\n",
      "Epoch: 39/50... Step: 18900... Loss: 1.4298...\n",
      "Epoch: 39/50... Step: 19000... Loss: 1.4247...\n",
      "Epoch: 39/50... Step: 19100... Loss: 1.3983...\n",
      "Epoch: 39/50... Step: 19200... Loss: 1.4809...\n",
      "Exception at index: 494\n",
      "Epoch: 40/50... Step: 19300... Loss: 1.4394...\n",
      "Epoch: 40/50... Step: 19400... Loss: 1.4362...\n",
      "Epoch: 40/50... Step: 19500... Loss: 1.3556...\n",
      "Epoch: 40/50... Step: 19600... Loss: 1.5297...\n",
      "Epoch: 40/50... Step: 19700... Loss: 1.4288...\n",
      "Exception at index: 494\n",
      "Epoch: 41/50... Step: 19800... Loss: 1.3938...\n",
      "Epoch: 41/50... Step: 19900... Loss: 1.4303...\n",
      "Epoch: 41/50... Step: 20000... Loss: 1.5840...\n",
      "Epoch: 41/50... Step: 20100... Loss: 1.4019...\n",
      "Epoch: 41/50... Step: 20200... Loss: 1.3518...\n",
      "Exception at index: 494\n",
      "Epoch: 42/50... Step: 20300... Loss: 1.5497...\n",
      "Epoch: 42/50... Step: 20400... Loss: 1.8430...\n",
      "Epoch: 42/50... Step: 20500... Loss: 1.6137...\n",
      "Epoch: 42/50... Step: 20600... Loss: 1.5550...\n",
      "Epoch: 42/50... Step: 20700... Loss: 1.3331...\n",
      "Exception at index: 494\n",
      "Epoch: 43/50... Step: 20800... Loss: 1.5567...\n",
      "Epoch: 43/50... Step: 20900... Loss: 1.5539...\n",
      "Epoch: 43/50... Step: 21000... Loss: 1.5279...\n",
      "Epoch: 43/50... Step: 21100... Loss: 1.5001...\n",
      "Epoch: 43/50... Step: 21200... Loss: 1.4826...\n",
      "Exception at index: 494\n",
      "Epoch: 44/50... Step: 21300... Loss: 1.5010...\n",
      "Epoch: 44/50... Step: 21400... Loss: 1.4267...\n",
      "Epoch: 44/50... Step: 21500... Loss: 1.5522...\n",
      "Epoch: 44/50... Step: 21600... Loss: 1.4257...\n",
      "Epoch: 44/50... Step: 21700... Loss: 1.4696...\n",
      "Exception at index: 494\n",
      "Epoch: 45/50... Step: 21800... Loss: 1.5531...\n",
      "Epoch: 45/50... Step: 21900... Loss: 1.4704...\n",
      "Epoch: 45/50... Step: 22000... Loss: 1.5160...\n",
      "Epoch: 45/50... Step: 22100... Loss: 1.4654...\n",
      "Epoch: 45/50... Step: 22200... Loss: 1.4318...\n",
      "Exception at index: 494\n",
      "Epoch: 46/50... Step: 22300... Loss: 1.4065...\n",
      "Epoch: 46/50... Step: 22400... Loss: 1.4874...\n",
      "Epoch: 46/50... Step: 22500... Loss: 1.5020...\n",
      "Epoch: 46/50... Step: 22600... Loss: 1.4122...\n",
      "Epoch: 46/50... Step: 22700... Loss: 1.5165...\n",
      "Exception at index: 494\n",
      "Epoch: 47/50... Step: 22800... Loss: 1.5093...\n",
      "Epoch: 47/50... Step: 22900... Loss: 1.4669...\n",
      "Epoch: 47/50... Step: 23000... Loss: 1.5046...\n",
      "Epoch: 47/50... Step: 23100... Loss: 1.4430...\n",
      "Epoch: 47/50... Step: 23200... Loss: 1.3633...\n",
      "Exception at index: 494\n",
      "Epoch: 48/50... Step: 23300... Loss: 1.5107...\n",
      "Epoch: 48/50... Step: 23400... Loss: 1.4407...\n",
      "Epoch: 48/50... Step: 23500... Loss: 1.4436...\n",
      "Epoch: 48/50... Step: 23600... Loss: 1.3954...\n",
      "Epoch: 48/50... Step: 23700... Loss: 1.3892...\n",
      "Exception at index: 494\n",
      "Epoch: 49/50... Step: 23800... Loss: 1.5468...\n",
      "Epoch: 49/50... Step: 23900... Loss: 1.4706...\n",
      "Epoch: 49/50... Step: 24000... Loss: 1.4212...\n",
      "Epoch: 49/50... Step: 24100... Loss: 1.4814...\n",
      "Epoch: 49/50... Step: 24200... Loss: 1.4074...\n",
      "Exception at index: 494\n",
      "Epoch: 50/50... Step: 24300... Loss: 1.4620...\n",
      "Epoch: 50/50... Step: 24400... Loss: 1.3423...\n",
      "Epoch: 50/50... Step: 24500... Loss: 1.4303...\n",
      "Epoch: 50/50... Step: 24600... Loss: 1.3706...\n",
      "Epoch: 50/50... Step: 24700... Loss: 1.4049...\n",
      "Exception at index: 494\n"
     ]
    }
   ],
   "source": [
    "train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdjxGRIo1aVs"
   },
   "outputs": [],
   "source": [
    "def save_weights(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_weights(model, path, test=False):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    if test:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_aOjiNDC0-V"
   },
   "outputs": [],
   "source": [
    "save_weights(model, '/content/drive/My Drive/Colab Notebooks/Practice/CharacterGeneration/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9s2Od2vC_BD"
   },
   "outputs": [],
   "source": [
    "train_on_gpu = True\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "    ''' Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "    '''\n",
    "    net.eval()\n",
    "    # tensor inputs\n",
    "    x = np.array([[char_to_int[char]]])\n",
    "    x = one_hot_encoder(x)\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() # move to cpu\n",
    "    \n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(printable))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "    \n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "    \n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return int_to_char[char], h\n",
    "\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "fES6rCjiE1BJ",
    "outputId": "62a02578-e0c7-42f3-aa26-8662c8213c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They will not always be the chest. The\n",
      "clothing of the station of the same conception of something\n",
      "and the more\n",
      "and the candles in a present, and his husband\n",
      "had surprised and\n",
      "say and so that he seemed a communication in the\n",
      "command of the morning of\n",
      "the first and character. Hisstrength of the\n",
      "campaign to the same\n",
      "side of its father. He says something at the freedom and son, with a smell of horses at all a servants the\n",
      "same changing\n",
      "his, he as it may\n",
      "have seen to\n",
      "his finger and striking such power of their patted to several pattend in the position of\n",
      "the\n",
      "first period. And any store of\n",
      "herself were askants. His faces\n",
      "which she had no\n",
      "son instead of significance of striking how he was strange, and that such a music carried\n",
      "and a little still\n",
      "son, and he\n",
      "had been\n",
      "as his ships, the movement is\n",
      "not a present to him to\n",
      "be desperately said that they were not\n",
      "the\n",
      "country to a present of her house.\n",
      "\n",
      "\"A character, and the son and\n",
      "some officers and hussars and told the prisoners, how if was had straig\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, prime='They', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGgFF31eE9ry"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "character_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
